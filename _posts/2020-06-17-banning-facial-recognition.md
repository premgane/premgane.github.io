---
layout:     post
title:      "It took two years to move the Overton Window on facial recognition"
date:       2020-06-17 15:30:00
summary:    "Amazon recently stopped providing facial recognition software to law enforcement after 2 years of research and activism."
tags:       machine-learning ethics ai
---

Amazon recently stopped providing facial recognition software to law enforcement after 2 years of research and activism.

[This MIT Tech Review article](https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/) and others like it have been making the rounds recently. Basically, it outlines the journey that an idea takes from research[^1] into having real-world effects. In retrospect, it seems obvious -- facial recognition is unproven, and of course it should not be used in law enforcement. Be careful, though, because this is most likely a case of hindsight bias[^2].

In the last two years, it seemed inevitable that facial recognition was the way of the future. The prevailing narrative was that companies producing these facial recognition products would continue to make improvements to their AI models and these biases would eventually disappear on their own. Take a look at [this article from April 2019.](https://www.nytimes.com/2019/04/03/technology/amazon-facial-recognition-technology.html)

> A day after this article was published, an Amazon spokeswoman responded, saying that the company had updated its Rekognition service since the M.I.T. researchers completed their study and that it had found no differences in error rates by gender and race when running similar tests.

Through popular journalism and relentless activism[^3] that exposed how this technology is being used, the [Overton Window](https://en.wikipedia.org/wiki/Overton_window), or the range of acceptable views in the mainstream, shifted over time. We have now entered into a space where it's more OK to question whether this technology should be used in law enforcement contexts.

I recommend [this interview](https://www.youtube.com/watch?v=uYlR3OIUQx4) with one of the original authors of the MIT paper, Timnit Gebru, where she explains in simple terms the consequences of biased facial recognition systems.

---

[^1]: [This is the original 2018 paper](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) published in the Proceedings of Machine Learning Research. [Here's the New York Times article about it](https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html), written for a broader audience.

[^2]: Hindsight bias refers to the common tendency for people to perceive events that have already occurred as having been more predictable than they actually were before the events took place. ([Wikipedia](https://en.wikipedia.org/wiki/Hindsight_bias))

[^3]: See [this article](https://www.technologyreview.com/2020/06/05/1002709/the-activist-dismantling-racist-police-algorithms/) about an activist in LA who is trying to stop the LAPD from using unproven predictive policing technology.